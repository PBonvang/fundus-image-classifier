command =  TrainConvNet:DumpNodeInfo:Eval:Write

makeMode = false ; traceLevel = 1 ; deviceId = "auto"

rootDir = "." ; dataDir  = "$rootDir$" ; modelDir = "$rootDir$/Models"

modelPath = "$modelDir$/model128x128.cmf"

# Training action for a convolutional network
TrainConvNet = {
    action = "train"

    BrainScriptNetworkBuilder = {
        imageShape =  128:128:3
        labelDim = 2

        MySubSampleBN (x, depth, stride) =
        {
            s = Splice ((MaxPoolingLayer {(1:1), stride=(stride:stride)} (x) : ConstantTensor (0, (1:1:depth/stride))), axis = 3)  # sub-sample and pad: [W x H x depth/2] --> [W/2 x H/2 x depth]
            b = BatchNormalizationLayer {spatialRank=2} (s)
        }.b
        MyConvBN (x, depth, stride) =
        {
            c = ConvolutionalLayer {depth, (3:3), pad=true, stride=(stride:stride), bias=false} (x)
            b = BatchNormalizationLayer {spatialRank=2} (c)
        }.b
        ResNetNode (x, depth) =
        {
            c1 = MyConvBN (x,  depth, 1)
            r1 = ReLU (c1)
            c2 = MyConvBN (r1, depth, 1)
            r  = ReLU (x + c2)
        }.r
        ResNetIncNode (x, depth) =
        {
            c1 = MyConvBN (x,  depth, 2)  # note the 2
            r1 = ReLU (c1)
            c2 = MyConvBN (r1, depth, 1)

            xs = MySubSampleBN (x, depth, 2)

            r  = ReLU (xs + c2)
        }.r
        ResNetNodeStack (x, depth, L) =
        {
            r = if L == 0
                then x
                else ResNetNode (ResNetNodeStack (x, depth, L-1), depth)
        }.r
        model (features) =
        {
            conv1 = ReLU (MyConvBN (features, 16, 1))
            rn1   = ResNetNodeStack (conv1, 16, 6)  ##### replaced

            rn2_1 = ResNetIncNode (rn1, 32)
            rn2   = ResNetNodeStack (rn2_1, 32, 2)  ##### replaced

            rn3_1 = ResNetIncNode (rn2, 64)
            rn3   = ResNetNodeStack (rn3_1, 64, 2)  ##### replaced
			
			d1_d = Dropout (rn3)

            pool = AveragePoolingLayer {(8:8)} (d1_d)

            z = LinearLayer {labelDim} (pool)
        }.z

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model (features)

        # connect to system
        ce       = CrossEntropyWithSoftmax (labels, z)
        errs     = ClassificationError     (labels, z)

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    }

    SGD = {
        epochSize = 0   
        maxEpochs = 500 ; minibatchSize = 32
        learningRatesPerSample = 0.00004
        momentumAsTimeConstant = 1200
        L2RegWeight = 0.0001
		dropoutRate = 0*15:0.5
        firstMBsToShowResult = 10 ; numMBsToShowResult = 10000

        ParallelTrain = {
			parallelTrain = false
			syncPerfStats=10
            parallelizationMethod = "DataParallelSGD"
            parallelizationStartEpoch = 1
            distributedMBReading = true
            dataParallelSGD = { gradientBits = 2 } }
        }
        AutoAdjust = {
		
		autoAdjustLR = "adjustAfterEpoch"   # "none"  | "searchBeforeEpoch" | "adjustAfterEpoch"
        autoAdjustMinibatch = true

        # for autoAdjustLR = "adjustAfterEpoch":
        reduceLearnRateIfImproveLessThan = 0
        learnRateDecreaseFactor = 0.618
        increaseLearnRateIfImproveMoreThan = infinity
        learnRateIncreaseFactor = 1.382
        loadBestModel = false
        learnRateAdjustInterval = 5
        useCVSetControlLRIfCVExists = true
        useEvalCriterionControlLR = false
		
            autoAdjustMinibatch = true        # enable automatic growing of minibatch size
            minibatchSizeTuningFrequency = 1 # try to enlarge after this many epochs
            numMiniBatch4LRSearch = 200
            minibatchSizeTuningMax =128     # out of memory above this
        }
    }

    reader = {
        verbosity = 0 ; randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "train.txt"
            input = {
                features = { transforms = (   
				{ type = "Crop" ; cropType = "RandomSide" ; sideRatio = 0.8 ; jitterType = "UniRatio" ; brightnessRadius=0.2;contrastRadius=0.2;saturationRadius=0.4} :
                    { type = "Scale" ; width = 128 ; height = 128 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Transpose" }
                )}
                labels = { labelDim = 2; format = "dense" }
            }
        })
    }
}
DumpNodeInfo = {
    action = "dumpNode"
    printValues = false
	printMetadata=true
	outputFile="nodedump.txt"
}
# Write action
Write = {
    action = "write"    
    outputPath = "outputDump.txt" 
	minibatchSize = 32
    reader = {
        verbosity = 0 ;
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "test.txt"
            input = {
                features = { transforms = (
                   { type = "Scale" ; width = 128 ; height = 128 ; channels = 3 ; interpolations = "linear" } :
                   { type = "Transpose" }
                )}
                labels = { labelDim = 2; format = "dense"}
            }
        })
    }
}

# Eval action
Eval = {
    action = "eval"    
    evalNodeNames = errs
	minibatchSize = 32
    reader = {
        verbosity = 0 ; randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "test.txt"
            input = {
                features = { transforms = (
                   { type = "Scale" ; width = 128 ; height = 128 ; channels = 3 ; interpolations = "linear" } :
                   { type = "Transpose" }
                )}
                labels = { labelDim = 2; format = "dense"}
            }
        })
    }
}
